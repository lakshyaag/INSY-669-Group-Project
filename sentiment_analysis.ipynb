{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysentiment2 as ps\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rich import print\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import pipeline\n",
    "\n",
    "from sentibignomics import senti_bignomics\n",
    "\n",
    "%load_ext rich\n",
    "\n",
    "\n",
    "load_dotenv(find_dotenv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = os.listdir(\"./extracted/\")\n",
    "\n",
    "docs = {\n",
    "    doc.split(\"_\")[0]: open(f\"./extracted/{doc}\", \"r\").read()\n",
    "    for doc in documents_list\n",
    "    if doc.endswith(\".txt\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at one example document\n",
    "print(docs[\"AAPL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting documents that have less than 1000 characters\n",
    "doc_lengths = {k: len(v) for k, v in docs.items()}\n",
    "print([(k, v) for k, v in doc_lengths.items() if v < 1000])\n",
    "docs_to_remove = [k for k, v in doc_lengths.items() if v < 1000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above tickers do not have relevant content in their MD&A, possibly due to cross-reference to other sections or unavailability of the documents. We drop these tickers from the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in docs_to_remove:\n",
    "    docs.pop(key, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.update([\"table_end\", \"table_start\"])\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "        self.stop_words.update([\"table_end\", \"table_start\"])\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Clean text\n",
    "        text = text.lower()  # Lowercase\n",
    "        text = re.sub(\n",
    "            r\"\\$\\d+(\\.\\d+)?(m| million| billion)?\", \" \", text\n",
    "        )  # Remove dollar amounts\n",
    "        text = re.sub(r\"\\d+(\\.\\d+)?%\", \" \", text)  # Remove percentages\n",
    "        text = re.sub(r\"\\b\\d+(\\.\\d+)?\\b\", \" \", text)  # Remove standalone numbers\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "\n",
    "        # Tokenize and remove stopwords\n",
    "        tokens = [\n",
    "            word for word in nltk.word_tokenize(text) if word not in self.stop_words\n",
    "        ]\n",
    "\n",
    "        # Lemmatize\n",
    "        lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        return lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the preprocessing pipeline as follows:\n",
    "- Convert to lowercase\n",
    "- Remove mention of dollar amounts\n",
    "- Remove percentages\n",
    "- Remove standalone numbers\n",
    "- Remove extra whitespaces and punctuations\n",
    "- Tokenize by word and remove stopwords\n",
    "- Lemmatize words using WordNetLemmatizer\n",
    "\n",
    "Since the focus is on a qualitative analysis of the MD&A, we remove dollar amounts, percenages and numbers as they are not relevant for the analysis. We also remove stopwords and lemmatize the words to reduce the dimensionality of the data and to focus on the content of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return \" \".join(tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=5,\n",
    "    max_features=2000,\n",
    "    preprocessor=preprocess_text,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the TF-IDF values\n",
    "tf_idf_df = pd.DataFrame(\n",
    "    tf_idf.todense(),\n",
    "    columns=tf_idf_vectorizer.get_feature_names_out(),\n",
    "    index=docs.keys(),\n",
    ")\n",
    "\n",
    "# Add a row for the document frequency\n",
    "tf_idf_df.loc[\"0_DOC_FREQ\"] = (tf_idf_df > 0).sum()\n",
    "tf_idf_df.sort_index().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 50 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf_idf_df.T.sort_values(\"0_DOC_FREQ\", ascending=False).head(50).index)\n",
    "tf_idf_df.drop(\"0_DOC_FREQ\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 terms by TF-IDF value for each document\n",
    "top_tfidf = (\n",
    "    (\n",
    "        tf_idf_df.stack()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            columns={\n",
    "                0: \"tfidf\",\n",
    "                \"level_0\": \"document\",\n",
    "                \"level_1\": \"term\",\n",
    "                \"level_2\": \"term\",\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .sort_values(by=[\"document\", \"tfidf\"], ascending=[True, False])\n",
    "    .groupby([\"document\"])\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "top_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_tfidf.copy()\n",
    "top_tfidf_plusRand[\"tfidf\"] = (\n",
    "    top_tfidf_plusRand[\"tfidf\"] + np.random.rand(top_tfidf.shape[0]) * 0.0001\n",
    ")\n",
    "\n",
    "# Create a base chart\n",
    "base = (\n",
    "    alt.Chart(top_tfidf_plusRand)\n",
    "    .encode(x=\"rank:O\", y=\"document:N\")\n",
    "    .transform_window(\n",
    "        rank=\"rank()\",\n",
    "        sort=[alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "        groupby=[\"document\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a heatmap of the top 10 terms by TF-IDF value for each document\n",
    "heatmap = base.mark_rect().encode(color=\"tfidf:Q\")\n",
    "# Add text labels for the terms\n",
    "text = base.mark_text(baseline=\"middle\").encode(\n",
    "    text=\"term:N\",\n",
    "    color=alt.condition(\n",
    "        alt.datum.tfidf >= 0.23, alt.value(\"white\"), alt.value(\"black\")\n",
    "    ),\n",
    ")\n",
    "\n",
    "(heatmap + text).properties(width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows the top 10 words with the highest TF-IDF scores for each document. We can see that the words are relevant to the content and industry of each stock ticker. For example, Boeing Airlines (BA) has words like \"aircraft\", \"airline\", \"contract\", while Tesla (TSLA) has words like \"automotive\", \"energy\", \"vehicle\".\n",
    "\n",
    "This indicates that the TF-IDF scores are capturing the relevant content of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform sentiment analysis using the lexicons, we tokenize each document into sentences, apply the preprocessing pipeline, and then calculate the sentiment scores for each sentence using the lexicons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VADER Dictionary (Generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VADER_res = []\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "for k, v in docs.items():\n",
    "    for sent in nltk.sent_tokenize(v):\n",
    "        scores = vader.polarity_scores(preprocess_text(sent))\n",
    "\n",
    "        VADER_res.append(\n",
    "            {\n",
    "                \"Document\": k,\n",
    "                \"Sentence\": sent,\n",
    "                \"VADER_Negative\": scores[\"neg\"],\n",
    "                \"VADER_Neutral\": scores[\"neu\"],\n",
    "                \"VADER_Positive\": scores[\"pos\"],\n",
    "                \"VADER_Compound\": scores[\"compound\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"Processed using VADER\")\n",
    "VADER_res = pd.DataFrame(VADER_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Financial Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loughran and McDonald Financial Sentiment Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loughran, Tim and McDonald, Bill, When is a Liability not a Liability? Textual Analysis, Dictionaries, and 10-Ks (March 4, 2010). Journal of Finance, Forthcoming, Available at SSRN: https://ssrn.com/abstract=1331573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ps.LM(tokenizer=tokenizer)\n",
    "\n",
    "LM_res = []\n",
    "\n",
    "for k, v in docs.items():\n",
    "    for sent in nltk.sent_tokenize(v):\n",
    "        scores = lm.get_score(lm.tokenize(sent))\n",
    "        LM_res.append(\n",
    "            {\n",
    "                \"Document\": k,\n",
    "                \"Sentence\": sent,\n",
    "                \"LM_Positive\": scores[\"Positive\"],\n",
    "                \"LM_Negative\": scores[\"Negative\"],\n",
    "                \"LM_Polarity\": scores[\"Polarity\"],\n",
    "                \"LM_Subjectivity\": scores[\"Subjectivity\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Processed using LM financial dictionary\")\n",
    "LM_res = pd.DataFrame(LM_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SentiBigNomics (VADER-based)\n",
    "\n",
    "Consoli, S., Barbaglia, L., & Manzan, S. (2022). Fine-grained, aspect-based sentiment analysis on economic and financial lexicon. Knowledge-Based Systems, 247, 108781. https://doi.org/10.1016/j.knosys.2022.108781"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SBN_res = []\n",
    "sbn_vader = SentimentIntensityAnalyzer()\n",
    "sbn_vader.lexicon.update(senti_bignomics)\n",
    "\n",
    "for k, v in docs.items():\n",
    "    for sent in nltk.sent_tokenize(v):\n",
    "        scores = sbn_vader.polarity_scores(preprocess_text(sent))\n",
    "\n",
    "        SBN_res.append(\n",
    "            {\n",
    "                \"Document\": k,\n",
    "                \"Sentence\": sent,\n",
    "                \"SBN_Negative\": scores[\"neg\"],\n",
    "                \"SBN_Neutral\": scores[\"neu\"],\n",
    "                \"SBN_Positive\": scores[\"pos\"],\n",
    "                \"SBN_Compound\": scores[\"compound\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"Processed using SentiBigNomics\")\n",
    "\n",
    "SBN_res = pd.DataFrame(SBN_res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained model from HuggingFace\n",
    "\n",
    "[FinBERT](https://huggingface.co/ProsusAI/finbert): Araci, D. (2019). FinBERT: Financial Sentiment Analysis with Pre-trained Language Models. arXiv (Cornell University). https://doi.org/10.48550/arxiv.1908.10063\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\", device=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_results = []\n",
    "\n",
    "for ticker, doc in docs.items():\n",
    "    for sent in nltk.sent_tokenize(doc):\n",
    "        sentiment = pipe(\n",
    "            sent, padding=True, truncation=True, max_length=512, top_k=None\n",
    "        )\n",
    "\n",
    "        finbert_results.append(\n",
    "            {\"Ticker\": ticker, \"Sentence\": sent, \"Sentiment\": sentiment}\n",
    "        )\n",
    "\n",
    "    print(f\"Processed {ticker} using FinBERT\")\n",
    "\n",
    "print(\"Processed using FinBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_results_df = pd.json_normalize(\n",
    "    finbert_results, record_path=\"Sentiment\", meta=[\"Ticker\", \"Sentence\"]\n",
    ").pivot_table(\n",
    "    values=\"score\",\n",
    "    index=[\"Ticker\", \"Sentence\"],\n",
    "    columns=\"label\",\n",
    ")\n",
    "\n",
    "finbert_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert_results_df.groupby(\"Ticker\")[['negative', 'neutral', 'positive']].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID-related disclosure frequency\n",
    "\n",
    "Dutta, S., Kumar, A., Pant, P., Walsh, C., & Dutta, M. (2023). Using 10-K text to gauge COVID-related corporate disclosure. PLOS ONE, 18(3), e0283138. https://doi.org/10.1371/journal.pone.0283138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words related to covid from paper\n",
    "covid_word_list = \"\"\"pandemic\n",
    "epidemic\n",
    "contagious\n",
    "disease\n",
    "infectious\n",
    "coronavirus\n",
    "covid\n",
    "strain\n",
    "outbreak\n",
    "resurgence\n",
    "health\n",
    "crisis\n",
    "\"\"\"\n",
    "\n",
    "covid_word_list = covid_word_list.split(\"\\n\")\n",
    "\n",
    "# getting lists of positive and negative words from VADER\n",
    "positive_words = [word for word in vader.lexicon if vader.lexicon[word] > 0]\n",
    "negative_words = [word for word in vader.lexicon if vader.lexicon[word] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for checking if a sentence contains any word from a list\n",
    "def sentence_contains_wordlist(words, wordlist):\n",
    "    return any(word in wordlist for word in words)\n",
    "\n",
    "\n",
    "covid_res = []\n",
    "\n",
    "for k, v in docs.items():\n",
    "    # initializing variables\n",
    "    sentences = nltk.sent_tokenize(v)\n",
    "    total_sentence_count = len(sentences)\n",
    "\n",
    "    # calculating contextual and covid-related disclosure frequencies\n",
    "    for sentence in sentences:\n",
    "        contextual_positive_freq = 0\n",
    "        contextual_negative_freq = 0\n",
    "        covid_freq = 0\n",
    "\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "\n",
    "        if sentence_contains_wordlist(tokens, covid_word_list):\n",
    "            covid_freq += 1\n",
    "\n",
    "        else:\n",
    "            covid_res.append(\n",
    "                {\n",
    "                    \"Document\": k,\n",
    "                    \"Sentence\": sentence,\n",
    "                    \"COVID_Related_Frequency\": covid_freq,\n",
    "                    \"COVID_Contextual_Positive_Frequency\": contextual_positive_freq,\n",
    "                    \"COVID_Contextual_Negative_Frequency\": contextual_negative_freq,\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        if sentence_contains_wordlist(tokens, positive_words):\n",
    "            contextual_positive_freq += 1\n",
    "\n",
    "        if sentence_contains_wordlist(tokens, negative_words):\n",
    "            contextual_negative_freq += 1\n",
    "\n",
    "        covid_res.append(\n",
    "            {\n",
    "                \"Document\": k,\n",
    "                \"Sentence\": sentence,\n",
    "                \"COVID_Related_Frequency\": covid_freq,\n",
    "                \"COVID_Contextual_Positive_Frequency\": contextual_positive_freq,\n",
    "                \"COVID_Contextual_Negative_Frequency\": contextual_negative_freq,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Processed using COVID-19 related words\")\n",
    "covid_res = pd.DataFrame(covid_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Lexicons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing LM, VADER, SentiBigNomics, and COVID-19 dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_results = pd.concat(\n",
    "    [\n",
    "        LM_res.set_index([\"Document\", \"Sentence\"]),\n",
    "        VADER_res.set_index([\"Document\", \"Sentence\"]),\n",
    "        SBN_res.set_index([\"Document\", \"Sentence\"]),\n",
    "        covid_res.set_index([\"Document\", \"Sentence\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").reset_index()\n",
    "\n",
    "nlp_results_stats = nlp_results.groupby(\"Document\").agg(\n",
    "    {\n",
    "        \"LM_Positive\": \"mean\",\n",
    "        \"LM_Negative\": \"mean\",\n",
    "        \"LM_Polarity\": \"mean\",\n",
    "        \"LM_Subjectivity\": \"mean\",\n",
    "        \"VADER_Negative\": \"mean\",\n",
    "        \"VADER_Neutral\": \"mean\",\n",
    "        \"VADER_Positive\": \"mean\",\n",
    "        \"VADER_Compound\": \"mean\",\n",
    "        \"SBN_Negative\": \"mean\",\n",
    "        \"SBN_Neutral\": \"mean\",\n",
    "        \"SBN_Positive\": \"mean\",\n",
    "        \"SBN_Compound\": \"mean\",\n",
    "        \"COVID_Related_Frequency\": \"sum\",\n",
    "        \"COVID_Contextual_Positive_Frequency\": \"sum\",\n",
    "        \"COVID_Contextual_Negative_Frequency\": \"sum\",\n",
    "        \"Sentence\": \"size\",\n",
    "    }\n",
    ")\n",
    "\n",
    "nlp_results_stats[\n",
    "    [\n",
    "        \"COVID_Related_Frequency\",\n",
    "        \"COVID_Contextual_Positive_Frequency\",\n",
    "        \"COVID_Contextual_Negative_Frequency\",\n",
    "    ]\n",
    "] = nlp_results_stats[\n",
    "    [\n",
    "        \"COVID_Related_Frequency\",\n",
    "        \"COVID_Contextual_Positive_Frequency\",\n",
    "        \"COVID_Contextual_Negative_Frequency\",\n",
    "    ]\n",
    "].div(nlp_results_stats[\"Sentence\"], axis=0)\n",
    "\n",
    "nlp_results_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding FinBERT results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = (\n",
    "    finbert_results_df.groupby(\"Ticker\")[[\"negative\", \"neutral\", \"positive\"]]\n",
    "    .agg(\"mean\")\n",
    "    .rename_axis(\"Document\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"negative\": \"FinBERT_Negative\",\n",
    "            \"neutral\": \"FinBERT_Neutral\",\n",
    "            \"positive\": \"FinBERT_Positive\",\n",
    "        }\n",
    "    )\n",
    "    .join(nlp_results_stats)\n",
    ")\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = results_df.drop(columns=\"Sentence\").corr()\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing sentiment score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df.iloc[:5].plot(\n",
    "    kind=\"bar\",\n",
    "    y=[\"FinBERT_Positive\", \"LM_Positive\", \"VADER_Positive\", \"SBN_Positive\", \"COVID_Contextual_Positive_Frequency\"],\n",
    "    figsize=(20, 10),\n",
    ")\n",
    "ax.set_ylabel(\"Positive Sentiment Score\")\n",
    "ax.set_title(\"Sentiment Score comparison for first 5 companies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df.iloc[:5].plot(\n",
    "    kind=\"bar\",\n",
    "    y=[\"FinBERT_Negative\", \"LM_Negative\", \"VADER_Negative\", \"SBN_Negative\", \"COVID_Contextual_Negative_Frequency\"],\n",
    "    figsize=(20, 10),\n",
    ")\n",
    "ax.set_ylabel(\"Negative Sentiment Score\")\n",
    "ax.set_title(\"Sentiment Score comparison for first 5 companies\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking COVID-19 mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.query(\"COVID_Related_Frequency == 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All MD&A sections mention terms related to COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Stock Prices with Results from Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Stock Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = pd.read_csv(\"./data/stock_prices.csv\", parse_dates=[\"Date\"])\n",
    "stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Industry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_data = pd.read_csv(\"./data/constituents.csv\")\n",
    "\n",
    "ticker_data[\"Symbol\"] = ticker_data[\"Symbol\"].str.replace(\".\", \"-\")\n",
    "\n",
    "ticker_data = (\n",
    "    ticker_data.set_index(\"Symbol\").rename_axis(\"Ticker\").join(results_df, how=\"inner\")\n",
    ")[[\"Name\", \"Sector\"]].sort_index()\n",
    "\n",
    "ticker_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the yearly price change for each ticker\n",
    "stock_df[\"Year\"] = stock_df[\"Date\"].dt.year\n",
    "\n",
    "price_change_df = stock_df.pivot(\n",
    "    index=\"Ticker\", columns=\"Year\", values=\"Close\"\n",
    ").pct_change(axis=1)\n",
    "\n",
    "# Reset index and prepare for merging with the result\n",
    "price_change_df = price_change_df.reset_index()\n",
    "price_change_df = price_change_df.drop(columns=2020)\n",
    "price_change_df.columns = [\"Ticker\", \"Price_Change_%\"]\n",
    "price_change_df = price_change_df.set_index(\"Ticker\").rename_axis(\"Document\")\n",
    "\n",
    "merged_df = results_df.join(price_change_df).join(ticker_data).reset_index()\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER vs. Stock Price Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a color map based on the Price Change %\n",
    "colors = merged_df[\"Price_Change_%\"].apply(lambda x: \"red\" if x < 0 else \"green\")\n",
    "\n",
    "plt.scatter(\n",
    "    merged_df[\"VADER_Compound\"],\n",
    "    merged_df[\"Price_Change_%\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    label=[\"Negative Change\", \"Positive Change\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Compound Sentiment Score\")\n",
    "plt.ylabel(\"Stock Price Change (%)\")\n",
    "plt.title(\"VADER Compound Sentiment Score vs. Stock Price Change\")\n",
    "\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"./plots/VADER_Compound_vs_Price_Change.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loughran and McDonald vs. Stock Price Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scatter plot of the loughran and mcdonald dictionary and the price change\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a color map based on the Price Change %\n",
    "colors = merged_df[\"Price_Change_%\"].apply(lambda x: \"red\" if x < 0 else \"green\")\n",
    "\n",
    "plt.scatter(\n",
    "    merged_df[\"LM_Polarity\"],\n",
    "    merged_df[\"Price_Change_%\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    label=[\"Negative Change\", \"Positive Change\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Loughran and McDonald Polarity Score\")\n",
    "plt.ylabel(\"Stock Price Change (%)\")\n",
    "plt.title(\"Loughran and McDonald Polarity Score vs. Stock Price Change\")\n",
    "\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"./plots/LM_Compound_vs_Price_Change.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiBigNomics vs. Stock Price Change\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a color map based on the Price Change %\n",
    "colors = merged_df[\"Price_Change_%\"].apply(lambda x: \"red\" if x < 0 else \"green\")\n",
    "\n",
    "plt.scatter(\n",
    "    merged_df[\"SBN_Compound\"],\n",
    "    merged_df[\"Price_Change_%\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    label=[\"Negative Change\", \"Positive Change\"],\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Compound Sentiment Score\")\n",
    "plt.ylabel(\"Stock Price Change (%)\")\n",
    "plt.title(\"SBN Compound Sentiment Score vs. Stock Price Change\")\n",
    "\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"./plots/SBN_Compound_vs_Price_Change.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinBERT vs. Stock Price Change\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most sentences have a neutral sentiment, we denote an overall document as \"positive\" if the average positive sentiment score is greater than the average negative sentiment score, and vice versa. We then calculate the stock price change for each ticker and compare it with the sentiment analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"FinBERT_Class\"] = (\n",
    "    merged_df[[\"FinBERT_Negative\", \"FinBERT_Positive\"]]\n",
    "    .idxmax(axis=1)\n",
    "    .str.split(\"_\")\n",
    "    .str[1]\n",
    ")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for FinBERT both positive and negative sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a color map based on the Price Change %\n",
    "colors = merged_df[\"Price_Change_%\"].apply(lambda x: \"red\" if x < 0 else \"green\")\n",
    "\n",
    "plt.scatter(\n",
    "    merged_df[\"FinBERT_Positive\"],\n",
    "    merged_df[\"Price_Change_%\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    label=[\"Negative Change\", \"Positive Change\"],\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Positive Sentiment Score\")\n",
    "plt.ylabel(\"Stock Price Change (%)\")\n",
    "plt.title(\"FinBERT Positive Sentiment Score vs. Stock Price Change\")\n",
    "\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"./plots/FinBERT_Positive_vs_Price_Change.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for FinBERT both positive and negative sentiment scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Create a color map based on the Price Change %\n",
    "colors = merged_df[\"Price_Change_%\"].apply(lambda x: \"red\" if x < 0 else \"green\")\n",
    "\n",
    "plt.scatter(\n",
    "    merged_df[\"FinBERT_Negative\"],\n",
    "    merged_df[\"Price_Change_%\"],\n",
    "    c=colors,\n",
    "    alpha=0.5,\n",
    "    label=[\"Negative Change\", \"Positive Change\"],\n",
    ")\n",
    "\n",
    "\n",
    "plt.xlabel(\"Negative Sentiment Score\")\n",
    "plt.ylabel(\"Stock Price Change (%)\")\n",
    "plt.title(\"FinBERT Negative Sentiment Score vs. Stock Price Change\")\n",
    "\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"./plots/FinBERT_Negative_vs_Price_Change.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_count = (\n",
    "    merged_df.groupby(\"FinBERT_Class\")[\"Sector\"].value_counts().to_frame().reset_index()\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(\n",
    "    data=merged_df,\n",
    "    y=\"Sector\",\n",
    "    hue=\"FinBERT_Class\",\n",
    "    palette={\"Negative\": \"red\", \"Positive\": \"green\"},\n",
    ")\n",
    "\n",
    "plt.title(\"Industry Count by FinBERT Sentiment Class\")\n",
    "plt.ylabel(\"Industry\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.legend(title=\"FinBERT Sentiment Class\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./plots/Industry_Count_by_FinBERT_Sentiment_Class.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COVID related analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataset adn plot the average covid related frequency, contexual positive and negative by sector\n",
    "covid_contextual_freq = (\n",
    "    merged_df.groupby(\"Sector\")[\n",
    "        [\"COVID_Related_Frequency\", \"COVID_Contextual_Positive_Frequency\", \"COVID_Contextual_Negative_Frequency\"]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "covid_contextual_freq = pd.melt(\n",
    "    covid_contextual_freq,\n",
    "    id_vars=\"Sector\",\n",
    "    value_vars=[\n",
    "        \"COVID_Related_Frequency\",\n",
    "        \"COVID_Contextual_Positive_Frequency\",\n",
    "        \"COVID_Contextual_Negative_Frequency\",\n",
    "    ],\n",
    "    var_name=\"Frequency_Type\",\n",
    "    value_name=\"Frequency\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.barplot(\n",
    "    data=covid_contextual_freq,\n",
    "    x=\"Frequency\",\n",
    "    y=\"Sector\",\n",
    "    hue=\"Frequency_Type\",\n",
    "    palette=\"viridis\",\n",
    ")\n",
    "\n",
    "plt.title(\"Average COVID-19 Related and Contextual Frequency by Sector\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"Sector\")\n",
    "plt.legend(title=\"Frequency Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"./plots/Average_COVID-19_Related_and_Contextual_Frequency_by_Sector.jpg\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
