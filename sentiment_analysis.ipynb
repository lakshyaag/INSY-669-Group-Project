{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysentiment2 as ps\n",
    "import spacy\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from rich import print\n",
    "from textblob import TextBlob\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "%load_ext rich\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = os.listdir(\"./extracted/\")\n",
    "\n",
    "docs = {\n",
    "    doc.split(\"_\")[0]: open(f\"./extracted/{doc}\", \"r\").read()\n",
    "    for doc in documents_list\n",
    "    if doc.endswith(\".txt\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs['AAPL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('&#',' ')\n",
    "    text = text.replace(\"\\\\ \",'')\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def text_tokenize(text):\n",
    "    nltk_tokens_no_stopwords = nltk.word_tokenize(text)\n",
    "    return nltk_tokens_no_stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    nltk_tokens_no_stopwords = [word for word in text if word not in stopwords.words('english')]\n",
    "    text_no_stopwords = ' '.join(nltk_tokens_no_stopwords)\n",
    "    return text_no_stopwords\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    text_lemmatized = ' '.join([token.lemma_ for token in doc])\n",
    "    return text_lemmatized\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = text_tokenize(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loughran and McDonald Financial Sentiment Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = ps.LM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in docs.items():\n",
    "    print(\n",
    "        f\"[bold green]Sentiment for {k}[/bold green]\\n\",\n",
    "        lm.get_score(lm.tokenize(preprocessing_text(v))),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `TextBlob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in docs.items():\n",
    "    print(\n",
    "        f\"[bold green]Sentiment for {k}[/bold green]\\n\",\n",
    "        TextBlob(preprocessing_text(v)).sentiment,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "for k, v in docs.items():\n",
    "    print(\n",
    "        f\"[bold green]Sentiment for {k}[/bold green]\\n\",\n",
    "        vader.polarity_scores(preprocessing_text(v)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-trained model\n",
    "\n",
    "@misc{yang2020finbert,\n",
    "    title={FinBERT: A Pretrained Language Model for Financial Communications},\n",
    "    author={Yi Yang and Mark Christopher Siy UY and Allen Huang},\n",
    "    year={2020},\n",
    "    eprint={2006.08097},\n",
    "    archivePrefix={arXiv},\n",
    "    }\n",
    "\n",
    "Only allows 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    outputs = finbert(**inputs)[0]\n",
    "\n",
    "    labels = {0: \"neutral\", 1: \"positive\", 2: \"negative\"}\n",
    "\n",
    "    return (text, \"----\", labels[np.argmax(outputs.detach().numpy())])\n",
    "\n",
    "\n",
    "for k, v in docs.items():\n",
    "    print(\n",
    "        f\"[bold green]Sentiment for {k}[/bold green]\\n\",\n",
    "        get_predictions(preprocessing_text(v)[:512]),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('text_group')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e044e03a2de942234da23604a7406518ab30968c80bc249e47496025b36e532"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
